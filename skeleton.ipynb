{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef808767",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install qiskit torch numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde406b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 191\u001b[0m\n\u001b[0;32m    189\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(X_train)\n\u001b[0;32m    190\u001b[0m loss   \u001b[38;5;241m=\u001b[39m loss_fn(logits, Y_train)\n\u001b[1;32m--> 191\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# training accuracy\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kuift\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kuift\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kuift\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32mc:\\Users\\Kuift\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\function.py:307\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m     )\n\u001b[0;32m    306\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[36], line 132\u001b[0m, in \u001b[0;36mVQALayerFunction.backward\u001b[1;34m(ctx, grad_output)\u001b[0m\n\u001b[0;32m    129\u001b[0m     pub_minus \u001b[38;5;241m=\u001b[39m (create_vqa_circuit(x, th_minus), SparsePauliOp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZI\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    131\u001b[0m     exp_plus  \u001b[38;5;241m=\u001b[39m extract_expval(estimator\u001b[38;5;241m.\u001b[39mrun([pub_plus ])\u001b[38;5;241m.\u001b[39mresult()[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m--> 132\u001b[0m     exp_minus \u001b[38;5;241m=\u001b[39m extract_expval(\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpub_minus\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresult()[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    134\u001b[0m     grads[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (exp_plus \u001b[38;5;241m-\u001b[39m exp_minus)\n\u001b[0;32m    136\u001b[0m grad_weights \u001b[38;5;241m=\u001b[39m grad_output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m    137\u001b[0m     grads, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mweights\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    138\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Kuift\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\qiskit\\primitives\\statevector_estimator.py:135\u001b[0m, in \u001b[0;36mStatevectorEstimator.run\u001b[1;34m(self, pubs, precision)\u001b[0m\n\u001b[0;32m    132\u001b[0m coerced_pubs \u001b[38;5;241m=\u001b[39m [EstimatorPub\u001b[38;5;241m.\u001b[39mcoerce(pub, precision) \u001b[38;5;28;01mfor\u001b[39;00m pub \u001b[38;5;129;01min\u001b[39;00m pubs]\n\u001b[0;32m    134\u001b[0m job \u001b[38;5;241m=\u001b[39m PrimitiveJob(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run, coerced_pubs)\n\u001b[1;32m--> 135\u001b[0m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_submit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m job\n",
      "File \u001b[1;32mc:\\Users\\Kuift\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\qiskit\\primitives\\primitive_job.py:46\u001b[0m, in \u001b[0;36mPrimitiveJob._submit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JobError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrimitive job has been submitted already.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m executor \u001b[38;5;241m=\u001b[39m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=consider-using-with\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_future \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m executor\u001b[38;5;241m.\u001b[39mshutdown(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Kuift\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\thread.py:179\u001b[0m, in \u001b[0;36mThreadPoolExecutor.submit\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m w \u001b[38;5;241m=\u001b[39m _WorkItem(f, fn, args, kwargs)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_work_queue\u001b[38;5;241m.\u001b[39mput(w)\n\u001b[1;32m--> 179\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_adjust_thread_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[1;32mc:\\Users\\Kuift\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\thread.py:202\u001b[0m, in \u001b[0;36mThreadPoolExecutor._adjust_thread_count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m thread_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_thread_name_prefix \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    196\u001b[0m                          num_threads)\n\u001b[0;32m    197\u001b[0m t \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mThread(name\u001b[38;5;241m=\u001b[39mthread_name, target\u001b[38;5;241m=\u001b[39m_worker,\n\u001b[0;32m    198\u001b[0m                      args\u001b[38;5;241m=\u001b[39m(weakref\u001b[38;5;241m.\u001b[39mref(\u001b[38;5;28mself\u001b[39m, weakref_cb),\n\u001b[0;32m    199\u001b[0m                            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_work_queue,\n\u001b[0;32m    200\u001b[0m                            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initializer,\n\u001b[0;32m    201\u001b[0m                            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initargs))\n\u001b[1;32m--> 202\u001b[0m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads\u001b[38;5;241m.\u001b[39madd(t)\n\u001b[0;32m    204\u001b[0m _threads_queues[t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_work_queue\n",
      "File \u001b[1;32mc:\\Users\\Kuift\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:999\u001b[0m, in \u001b[0;36mThread.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m _limbo[\u001b[38;5;28mself\u001b[39m]\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m--> 999\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_started\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kuift\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\Kuift\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.primitives import StatevectorEstimator\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Prep Training Data (Breast Cancer Wisconsin)\n",
    "def extract_expval(pub_result):\n",
    "    \"\"\"Return a plain Python float for the first observable/parameter set,\n",
    "       whatever Qiskit version is installed.\"\"\"\n",
    "    if hasattr(pub_result, \"values\"):           # 0.46-style Estimator V1\n",
    "        return float(pub_result.values[0])\n",
    "    elif hasattr(pub_result, \"data\"):           # 0.47+ Estimator V2\n",
    "        return float(pub_result.data.evs.item())     # .item() -> scalar\n",
    "    else:\n",
    "        raise TypeError(\"Unrecognised estimator result structure.\")\n",
    "\n",
    "#Load the data file\n",
    "df = pd.read_csv(\"data\\\\wdbc.data\", header=None)\n",
    "\n",
    "#Assign column names\n",
    "columns = ['id', 'diagnosis'] + [f'feature_{i}' for i in range(1, 31)]\n",
    "df.columns = columns\n",
    "\n",
    "#Drop the ID column\n",
    "df = df.drop(columns=['id'])\n",
    "\n",
    "#Encode diagnosis: M = 1, B = 0\n",
    "df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "#Convert to numpy arrays\n",
    "X = df.drop(columns=['diagnosis']).values.astype(np.float32)\n",
    "Y = df['diagnosis'].values.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "#Normalize features manually (z-score)\n",
    "mean = X.mean(axis=0)\n",
    "std = X.std(axis=0)\n",
    "X = (X - mean) / std\n",
    "pca = PCA(n_components=2, whiten=True, random_state=1)\n",
    "X = pca.fit_transform(X).astype(np.float32)\n",
    "\n",
    "#Convert to torch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "#Manual train/test split (80/20)\n",
    "num_samples = X_tensor.shape[0]\n",
    "indices = torch.randperm(num_samples)\n",
    "\n",
    "split_idx = int(num_samples * 0.8)\n",
    "train_indices = indices[:split_idx]\n",
    "test_indices = indices[split_idx:]\n",
    "\n",
    "X_train = X_tensor[train_indices]\n",
    "Y_train = Y_tensor[train_indices]\n",
    "X_test = X_tensor[test_indices]\n",
    "Y_test = Y_tensor[test_indices]\n",
    "\n",
    "#VQA Circuit\n",
    "n_qubits = 2\n",
    "params = ParameterVector('theta', length=4)\n",
    "\n",
    "\n",
    "def create_vqa_circuit(inputs, weights):\n",
    "    \"\"\"\n",
    "    inputs  : 1-D numpy array of length 2  (classical features)\n",
    "    weights : 1-D numpy array of length 4  (trainable parameters)\n",
    "    \"\"\"\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "\n",
    "    # --- feature encoding (one RY per qubit) -------------\n",
    "    for q, x in enumerate(inputs):\n",
    "        qc.ry(float(x), q)\n",
    "\n",
    "    # --- simple entanglement -----------------------------\n",
    "    qc.cx(0, 1)\n",
    "\n",
    "    # --- variational layer -------------------------------\n",
    "    qc.ry(float(weights[0]), 0)\n",
    "    qc.rz(float(weights[1]), 0)\n",
    "    qc.ry(float(weights[2]), 1)\n",
    "    qc.rz(float(weights[3]), 1)\n",
    "\n",
    "    return qc\n",
    "\n",
    "\n",
    "#Qiskit StatevectorEstimator primitive\n",
    "estimator = StatevectorEstimator()\n",
    "observables = [SparsePauliOp(\"ZI\")]\n",
    "\n",
    "#PyTorch Custom Autograd Function For VQA Layer\n",
    "# --- 2. Autograd function with parameter-shift -----------\n",
    "\n",
    "class VQALayerFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_tensor, weights):\n",
    "        ctx.save_for_backward(input_tensor, weights)\n",
    "\n",
    "        pub = (create_vqa_circuit(input_tensor.numpy(),\n",
    "                                  weights.detach().numpy()),\n",
    "               SparsePauliOp(\"ZI\"))\n",
    "        job     = estimator.run([pub])          # â† reuse the global estimator\n",
    "        expval  = extract_expval(job.result()[0])\n",
    "\n",
    "        return torch.tensor([expval],\n",
    "                            dtype=torch.float32,\n",
    "                            device=input_tensor.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_tensor, weights = ctx.saved_tensors\n",
    "        x   = input_tensor.detach().numpy()\n",
    "        th0 = weights.detach().numpy()\n",
    "        sh  = np.pi / 2\n",
    "\n",
    "        grads = np.zeros_like(th0, dtype=np.float32)\n",
    "\n",
    "        for i in range(len(th0)):\n",
    "            th_plus, th_minus = th0.copy(), th0.copy()\n",
    "            th_plus[i]  += sh\n",
    "            th_minus[i] -= sh\n",
    "\n",
    "            pub_plus  = (create_vqa_circuit(x, th_plus),  SparsePauliOp(\"ZI\"))\n",
    "            pub_minus = (create_vqa_circuit(x, th_minus), SparsePauliOp(\"ZI\"))\n",
    "\n",
    "            exp_plus  = extract_expval(estimator.run([pub_plus ]).result()[0])\n",
    "            exp_minus = extract_expval(estimator.run([pub_minus]).result()[0])\n",
    "\n",
    "            grads[i] = 0.5 * (exp_plus - exp_minus)\n",
    "\n",
    "        grad_weights = grad_output.view(-1)[0] * torch.tensor(\n",
    "            grads, dtype=torch.float32, device=weights.device\n",
    "        )\n",
    "\n",
    "        return None, grad_weights\n",
    "\n",
    "\n",
    "\n",
    "#Quantum Layer as PyTorch Module\n",
    "class VQALayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(4)) #4 trainable parameters for the circuit\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.stack([VQALayerFunction.apply(x[i], self.weights) for i in range(x.size(0))]).view(-1, 1)\n",
    "\n",
    "#Full Hybrid Model\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.classical = nn.Linear(X_tensor.shape[1], 2) #Classic preprocessing layer\n",
    "        self.quantum = VQALayer() #VQA layer\n",
    "        self.output = nn.Linear(1, 1) #Final classical layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.classical(x)\n",
    "        x = torch.tanh(x)  #Activation before quantum layer\n",
    "        x = self.quantum(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model = HybridModel()\n",
    "pos_weight = torch.tensor([Y_train.mean().reciprocal()]) \n",
    "loss_fn    = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "# loss_fn = nn.BCELoss()\n",
    "#Training Loop\n",
    "# for epoch in range(50):\n",
    "#     optimizer.zero_grad()\n",
    "#     preds = model(X_train)\n",
    "#     loss = loss_fn(preds, Y_train)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     with torch.no_grad():\n",
    "#         acc = ((preds > 0.5).float() == Y_train).float().mean()\n",
    "#         print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f} | Accuracy: {acc.item()*100:.2f}%\")\n",
    "\n",
    "for epoch in range(70):\n",
    "    # ----------- train on the whole set (no mini-batch) ------------------\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(X_train)\n",
    "    loss   = loss_fn(logits, Y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # training accuracy\n",
    "    with torch.no_grad():\n",
    "        train_pred = sigmoid(logits) > 0.5\n",
    "        train_acc  = (train_pred == Y_train).float().mean().item() * 100\n",
    "\n",
    "    # ------------------- evaluate on the test set ------------------------\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_logits = model(X_test)\n",
    "        test_pred   = sigmoid(test_logits) > 0.5\n",
    "        test_acc    = (test_pred == Y_test).float().mean().item() * 100\n",
    "        test_loss   = loss_fn(test_logits, Y_test).item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d}  \"\n",
    "          f\"Train-loss {loss.item():.4f}  Train-acc {train_acc:5.1f}%   \"\n",
    "          f\"Test-loss {test_loss:.4f}  Test-acc {test_acc:5.1f}%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
